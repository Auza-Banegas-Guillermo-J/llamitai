{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9c6f7e-6871-4d85-ac74-1124d0359097",
   "metadata": {},
   "source": [
    "__LLAMA BOT ALPHA PREVIEW__ \\\n",
    "__by:__ Guillermo Javier Auza Banegas\\\n",
    "__Motivation:__ Skill Evaluation \\\n",
    "__Company:__ LlamitAI \\\n",
    "__Date:__ Feb 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20f6c0-5911-4084-9c9f-934d5955be34",
   "metadata": {},
   "source": [
    "For this notebook to work, ollama must be serving an LLM model and an embedding model, in this case deepseek and nomic respectively \\\n",
    "\\\n",
    "First of all, libraries must be imported, in this case langchain and its variants: community, llm, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97433c07-ced4-437f-bb7b-b6de75420117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c622a-b7dc-4816-87be-1c84b2043b1f",
   "metadata": {},
   "source": [
    "Now, we define the functions that will be used during our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c4fb94-d9bd-4327-8edf-57423a16828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path=\"./docs/txt/\"): # default path is a double folder with txt files\n",
    "    loader = DirectoryLoader(path, glob=\"*.txt\") # read all txt files in the path\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def chunk_splitter(docs): # argument previously split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter( \n",
    "        chunk_size = 750, \n",
    "        chunk_overlap = 300,\n",
    "        length_function = len,\n",
    "        add_start_index = True,\n",
    "        )\n",
    "    chunks = text_splitter.split_documents(docs) # turn each document into chunks using parameters from text splitter\n",
    "    return chunks\n",
    "\n",
    "def format_docs(docs): # turn documents into a single string\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8709d-4614-4502-a2a5-94648b8469a4",
   "metadata": {},
   "source": [
    "After declaring the functions, we can start defining the models and getting the embeddings from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6baed7d-3f34-498d-adca-3aae9bf43b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents() # load documents\n",
    "chunks = chunk_splitter(documents) # turn list of docs into list of chunks\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\") # load a model for embedding text\n",
    "vectorstore = Chroma.from_documents(documents = chunks, embedding = embedding_model) # store embeddings in a vector store\n",
    "\n",
    "model = OllamaLLM( # Define our LLM model and its parameters in this case, deepseek r1 with 14b params\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    temperature = 0,\n",
    "    max_tokens = None,\n",
    "    timeout = None,\n",
    "    max_retries = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eca8dc-633f-4f3a-880a-d67ad5fd7636",
   "metadata": {},
   "source": [
    "Now, lets define a template for our propts and for this example, a predefined question or petition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b10f23d-40c7-4183-b77b-389bc7a9573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Responde a la peticion cubierto en llaves de manera simple y concisa.\n",
    "Para esta tarea utiliza el contexto en sus llaves correspondientes.\n",
    "\n",
    "<contexto>\n",
    "contexto : {context}\n",
    "</contexto>\n",
    "\n",
    "<peticion>\n",
    "peticion: {question}\n",
    "</peticion>\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"¿Quiénes son los demandados?\n",
    "¿Quiénes son los demandantes?\n",
    "¿Quién es el juez encargado del caso?\n",
    "¿Que tipo de proceso es este?\n",
    "¿Cuando y donde sucedio la demanda?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ee56b-36ae-4d10-a2cc-7bdbdabfc488",
   "metadata": {},
   "source": [
    "Finally, lets define the chain of operations for the model so we can invoke a simplified a chain of operations for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df48527e-fd37-4e42-a197-00835337907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(question) # Search for similiraty in the embeddings from the question and our vector store\n",
    "context = format_docs(docs) # Setup the document chunk as a string for context\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) # create the prompt from our template with pending context and petition\n",
    "chain = prompt | model # establish the chain of operation from the propt to the model\n",
    "\n",
    "response = chain.invoke({\"context\": context, \"question\": question}) # invoke the chain of operations with determined context and petition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba978ef-64af-4d1d-aa2a-75fdec719289",
   "metadata": {},
   "source": [
    "At last, we can print the models output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b693d37-e3f7-4a81-a371-623411e85d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Bueno, tengo que responder a las preguntas de la petición utilizando el contexto proporcionado. Primero, identificaré quiénes son los demandados y los demandantes. En el contexto, veo que \"DEMANDANTE: 1. BANCO DEL PUEBLO S.A.\" y \"A: 1. RAFAEL PEREZ BLANCO CON C.I. 8765402-LP.\" así que estos son respectivamente los demandantes y los demandados.\n",
       "\n",
       "Luego, para saber quién es el juez encargado del caso, buscaré en el contexto la información sobre el juez. Veo que \"JUEZ PUBLICO CIVIL y COMERCIAL 3\" y el nombre de M Sc DAEN Fausto Calle M., así que eso responderá a quién es el juez.\n",
       "\n",
       "El tipo de proceso se menciona como \"PROCESO: CIVIL EJECUTIVO\", así que ese será el tipo de proceso.\n",
       "\n",
       "Para cuando y donde sucedió la demanda, en el contexto hay fechas y lugares. La sentencia inicial fue el 20 de septiembre de 2024, pero la petición menciona el 15 de enero de 2025, así que probablemente se refiere a cuando se presentó la solicitud o notificación. El lugar es La Paz, Bolivia.\n",
       "\n",
       "Ahora, organizaré las respuestas de manera clara y concisa utilizando las llaves correspondientes.\n",
       "</think>\n",
       "\n",
       "**Respuesta:**\n",
       "\n",
       "- **Demandados:** Rafael Pérez Blanco con CI 8765402-LP.\n",
       "- **Demandantes:** Banco del Pueblo S.A.\n",
       "- **Juez encargado:** Juez Público Civil y Comercial 3°, M.Sc. DAEN Fausto Calle M.\n",
       "- **Tipo de proceso:** Civil ejecutivo.\n",
       "- **Cuándo y dónde:** La demanda se presentó el 15 de enero de 2025 en la ciudad de La Paz, Bolivia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
