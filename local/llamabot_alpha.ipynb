{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9c6f7e-6871-4d85-ac74-1124d0359097",
   "metadata": {},
   "source": [
    "__LLAMA BOT ALPHA PREVIEW__ \\\n",
    "__by:__ Guillermo Javier Auza Banegas\\\n",
    "__Motivation:__ Skill Evaluation \\\n",
    "__Company:__ LlamitAI \\\n",
    "__Date:__ Feb 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20f6c0-5911-4084-9c9f-934d5955be34",
   "metadata": {},
   "source": [
    "For this notebook to work, ollama must be serving an LLM model and an embedding model, in this case deepseek and nomic respectively \\\n",
    "\\\n",
    "First of all, libraries must be imported, in this case langchain and its variants: community, llm, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97433c07-ced4-437f-bb7b-b6de75420117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c622a-b7dc-4816-87be-1c84b2043b1f",
   "metadata": {},
   "source": [
    "Now, we define the functions that will be used during our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c4fb94-d9bd-4327-8edf-57423a16828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path=\"./docs/txt/\"): # default path is a double folder with txt files\n",
    "    loader = DirectoryLoader(path, glob=\"*.txt\") # read all txt files in the path\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def chunk_splitter(docs): # argument previously split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter( \n",
    "        chunk_size = 750, \n",
    "        chunk_overlap = 300,\n",
    "        length_function = len,\n",
    "        add_start_index = True,\n",
    "        )\n",
    "    chunks = text_splitter.split_documents(docs) # turn each document into chunks using parameters from text splitter\n",
    "    return chunks\n",
    "\n",
    "def format_docs(docs): # turn documents into a single string\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8709d-4614-4502-a2a5-94648b8469a4",
   "metadata": {},
   "source": [
    "After declaring the functions, we can start defining the models and getting the embeddings from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6baed7d-3f34-498d-adca-3aae9bf43b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents() # load documents\n",
    "chunks = chunk_splitter(documents) # turn list of docs into list of chunks\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\") # load a model for embedding text\n",
    "vectorstore = Chroma.from_documents(documents = chunks, embedding = embedding_model) # store embeddings in a vector store\n",
    "\n",
    "model = OllamaLLM( # Define our LLM model and its parameters in this case, deepseek r1 with 14b params\n",
    "    model=\"deepseek-r1:14b\",\n",
    "    temperature = 0,\n",
    "    max_tokens = None,\n",
    "    timeout = None,\n",
    "    max_retries = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eca8dc-633f-4f3a-880a-d67ad5fd7636",
   "metadata": {},
   "source": [
    "Now, lets define a template for our propts and for this example, a predefined question or petition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b10f23d-40c7-4183-b77b-389bc7a9573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Responde a la peticion cubierto en llaves de manera simple y concisa.\n",
    "Para esta tarea utiliza el contexto en sus llaves correspondientes.\n",
    "\n",
    "<contexto>\n",
    "contexto : {context}\n",
    "</contexto>\n",
    "\n",
    "<peticion>\n",
    "peticion: {question}\n",
    "</peticion>\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"¿Quiénes son los demandados?\n",
    "¿Quiénes son los demandantes?\n",
    "¿Quién es el juez encargado del caso?\n",
    "¿Que tipo de proceso es este?\n",
    "¿Cuando y donde sucedio la demanda?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ee56b-36ae-4d10-a2cc-7bdbdabfc488",
   "metadata": {},
   "source": [
    "Finally, lets define the chain of operations for the model so we can invoke a simplified a chain of operations for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df48527e-fd37-4e42-a197-00835337907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(question) # Search for similiraty in the embeddings from the question and our vector store\n",
    "context = format_docs(docs) # Setup the document chunk as a string for context\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template) # create the prompt from our template with pending context and petition\n",
    "chain = prompt | model # establish the chain of operation from the propt to the model\n",
    "\n",
    "response = chain.invoke({\"context\": context, \"question\": question}) # invoke the chain of operations with determined context and petition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba978ef-64af-4d1d-aa2a-75fdec719289",
   "metadata": {},
   "source": [
    "Finally, lets see what the model respond to our question based on the following document:\\\n",
    "\\\n",
    "15-ENE 2025 9:14 \\\n",
    "TDJ \\\n",
    "TRIBU NAD DEPARTAMENTALL \\\n",
    "DE \\\n",
    "S \\\n",
    "JUSTICLA \\\n",
    "LAPAZ \\\n",
    "A \\\n",
    "112 \\\n",
    "ASFI \\\n",
    "15 ENE 2025 \\\n",
    "10791 \\\n",
    "ESTADO PLURINACIONAL DE BOLIVIA \\\n",
    "\"LP\" \\\n",
    "CIUDAD DE LA PAZ \\\n",
    "NUREJ: 204177105 \\\n",
    "AUTORIDAD DE SUPERVISION\\\n",
    "DEL SISTEMA FINANCIERO\\\n",
    "EXP: 196/24\\\n",
    "No. TRÁMITE\\\n",
    "Cite Of. 99/2024\\\n",
    "E\\\n",
    "T-1211991226\\\n",
    "E\\\n",
    "Lugar y fecha: La Paz, 25 de octubre de 2024\\\n",
    "Señores: AUTORIDAD DE SUPERVISIÓN DEL SISTEMA FINANCIERO - ASFI La Paz\\\n",
    "REF.: RETENCIÓN DE FONDOS A: 1. RAFAEL PEREZ BLANCO CON C.I. 8765402-LP.\\\n",
    "DEMANDANTE: 1. BANCO DEL PUEBLO S.A.\\\n",
    "PROCESO: CIVIL EJECUTIVO\\\n",
    "MONTO A RETENER: EN CAJA DE AHORRO, CUENTA CORRIENTE, DEPÓSITOS\\\n",
    "A PLAZO FIJO, TÍTULOS, VALORES Y OTROS SIMILARES, HASTA EL MONTO\\\n",
    "ADEUDADO DE Bs.- 15.010.00.- (SON QUINCE MIL DIEZ 00/100 BOLIVIANOS)\\\n",
    "Así se tiene ORDENADO mediante Sentencia inicial Nro. 756/2024 de fecha 20 de\\\n",
    "septiembre de 2024 a fs. 76 a 78 de obrados, siempre y cuando no se trate del pago\\\n",
    "de salarios, de ser así estese a las previsiones del Art. 48 parágrafo IV de la\\\n",
    "Constitución Política del Estado.\\\n",
    "FCM/CCC\\\n",
    "DEPARTAMENTAL\\\n",
    "JUZGADO\\\n",
    "PÚBLICO CIVILY\\\n",
    "\\\n",
    "COMERCIAL 3°\\\n",
    "M Sc DAEN Fausto Calle M.\\\n",
    "JUEZ PUBLICO CIVIL y COMERICIAL 3\\\n",
    "DEPARTAMENTO BE JUSTICIA\\\n",
    "SELLO DE JUZGADO\\\n",
    "FIRMADEOIUBZ\\\n",
    "Paz-Bolivia\\\n",
    "SELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b693d37-e3f7-4a81-a371-623411e85d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Bueno, tengo que responder a las preguntas de la petición utilizando el contexto proporcionado. Primero, identificaré cada pregunta y buscaré la información correspondiente en el contexto.\n",
       "\n",
       "1. **Demandados**: En el contexto se menciona \"RAFAEL PEREZ BLANCO\" como el demandado con su CI.\n",
       "2. **Demandantes**: El_demandante es \"BANCO DEL PUEBLO S.A.\" según el texto.\n",
       "3. **Juez encargado**: El juez es MSc DAEN Fausto Calle M., del Juzgado Público Civil y Comercial 3°.\n",
       "4. **Tipo de proceso**: Es un proceso \"CIVIL EJECUTIVO\".\n",
       "5. **Cuándo y dónde sucedió la demanda**: La sentencia fue el 20 de septiembre de 2024, en el Juzgado Público Civil y Comercial 3°.\n",
       "\n",
       "Ahora, estructuraré las respuestas de manera clara y concisa.\n",
       "</think>\n",
       "\n",
       "**Respuesta a la petición:**\n",
       "\n",
       "1. **Demandados**: Rafael Pérez Blanco, con CI 8765402-LP.\n",
       "2. **Demandantes**: Banco del Pueblo S.A.\n",
       "3. **Juez encargado**: MSc DAEN Fausto Calle M., Juzgado Público Civil y Comercial 3°.\n",
       "4. **Tipo de proceso**: Proceso civil ejecutivo.\n",
       "5. **Cuándo y dónde sucedió la demanda**: El proceso se tramitó en el Juzgado Público Civil y Comercial 3°, con una sentencia emitida el 20 de septiembre de 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
